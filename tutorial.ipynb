{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bafcc657",
   "metadata": {},
   "source": [
    " [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/LotsoTeddy/ark-samples/blob/main/tutorial.ipynb)\n",
    "\n",
    "\n",
    "\n",
    "<hr/>\n",
    "<img src=\"https://portal.volccdn.com/obj/volcfe/logo/appbar_logo_dark.2.svg?sanitize=true\" align=center>\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5cc44f8",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This is a **novice-friendly tutorial** for Volengine ARK SDK and API. This tutorial is able to help you to build your own intelligent applications through agent, knowledge base, and other amazing features.\n",
    "\n",
    "[Volengine ARK](https://www.volcengine.com/product/ark) provides a development platform with large model services, offering feature-rich, secure and price-competitive model calling services, as well as end-to-end functions such as model data, fine-tuning, reasoning, evaluation, and so on, to comprehensively guarantee your AI application development landing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a690eb12",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bc1bba",
   "metadata": {},
   "source": [
    "### Why ARK?\n",
    "\n",
    "ARK is a platform that supports multiple kinds of models running. Ark has the following advantages:\n",
    "\n",
    "- **Security and Mutual Trust**: Large model security and trust program strictly protects the model and information security of model providers and model users, click to view the white paper on security and mutual trust.\n",
    "- **Selected Models**: Supporting multi-industry models for various business scenarios, providing rich platform applications and tools to help you build your own innovative scenarios.\n",
    "- **Strong Arithmetic Power**: Based on the volcano's Wanka resource pool, we provide sufficient high-performance GPU resources to provide you with end-to-end modeling services including model fine-tuning, evaluation, and inference.\n",
    "- **Enterprise-level services**: provide professional service system support, professional product operation and sales delivery services to meet the needs of enterprise application construction and delivery."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ab1bdb",
   "metadata": {},
   "source": [
    "### Productions\n",
    "\n",
    "- Large models (e.g., Doubao-*, Deepseek-*, etc.)\n",
    "- Knowledge base\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0615505",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a225f331",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    " Install Volcengine ARK SDK and ARK Agent SDK via `pip`. \n",
    " \n",
    "- Source code of ARK SDK is available [here](https://github.com/volcengine/volcengine-python-sdk)\n",
    "- Source code of ARK Agent SDK is available [here](https://github.com/volcengine/ai-app-lab/tree/main/arkitect)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7240f703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for basic usage demos\n",
    "%pip install -q 'volcengine-python-sdk[ark]'\n",
    "\n",
    "# for agent demos\n",
    "%pip install -q arkitect==0.2.3 --index-url https://pypi.org/simple\n",
    "\n",
    "# for mcp-related demos\n",
    "%pip install -q fastmcp\n",
    "%pip install -q mcp_server_time\n",
    "\n",
    "# for RAG-related demos\n",
    "%pip install -q chromadb\n",
    "%pip install -q opensearch-py\n",
    "\n",
    "print(\"Install all packages done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83256f06",
   "metadata": {},
   "source": [
    "### Set API Key\n",
    "\n",
    "Before running this tutorial, you should generate your ARK API KEY (see [here](https://www.volcengine.com/docs/82379/1541594))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86675b9",
   "metadata": {},
   "source": [
    "#### Notebook\n",
    "\n",
    "In this tutorial, set `YOUR_ARK_API_KEY` as an environment and a global variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08be6e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"ARK_API_KEY\"] = \"YOUR_ARK_API_KEY\"  # <--- REPLACE\n",
    "ARK_API_KEY = os.environ[\"ARK_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dd1abb",
   "metadata": {},
   "source": [
    "#### Google Colab\n",
    "\n",
    "If you run this tutorial in Google Colab, you can set your ARK api key in your browser. Then run this code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73e2cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from google.colab import userdata\n",
    "\n",
    "os.environ[\"ARK_API_KEY\"] = userdata.get(\"ARK_API_KEY\")\n",
    "ARK_API_KEY = os.environ[\"ARK_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ee0a69",
   "metadata": {},
   "source": [
    "### Global Configs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa232445",
   "metadata": {},
   "source": [
    "In this tutorial, we set some constants here. You can change them to your own values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbde7006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for text processing\n",
    "DEFAULT_LLM = \"doubao-1.5-pro-32k-250115\"\n",
    "\n",
    "# for image understanding\n",
    "DEFAULT_VLM = \"doubao-1.5-vision-pro-32k-250115\"\n",
    "\n",
    "# for video generation\n",
    "VIDEO_GENERATION_LM = \"doubao-seaweed-241128\"\n",
    "\n",
    "# for text embedding, when building RAG\n",
    "# before using this model, you need to enable this model in ARK console\n",
    "EMBEDDING_MODEL = \"doubao-embedding-text-240715\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1798ed69",
   "metadata": {},
   "source": [
    "We process some log-related configs to remove useless log outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e28eaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import logging\n",
    "\n",
    "# ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# disable logs\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d662a02",
   "metadata": {},
   "source": [
    "## Quickstart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827b3453",
   "metadata": {},
   "source": [
    "Simplest, you can chat with a model by the chat completion interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd015ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from volcenginesdkarkruntime import Ark\n",
    "\n",
    "client = Ark(api_key=ARK_API_KEY)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=DEFAULT_LLM,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Slogan of Bytedance?\"}],\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95b8beb",
   "metadata": {},
   "source": [
    "Furthermore, you can send a *system prompt* by specifying the role as *system*, which can help you to control the behavior of the model. For example, you can use the system prompt to tell the model to do some translations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453accfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from volcenginesdkarkruntime import Ark\n",
    "\n",
    "client = Ark(api_key=ARK_API_KEY)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=DEFAULT_LLM,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"Translate the input text from English to Chinese, French, and Japanese.\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": \"Inspire Creativity, Enrich Life!\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dbee44",
   "metadata": {},
   "source": [
    "# Basic Usage\n",
    "\n",
    "This section introduces the basic usage and features of ARK SDK."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b5f935",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "ARK's model family includes a wide range of models. Here we list some primary models and its abilities:\n",
    "\n",
    "| Model ID                                      | Image Understanding | Video Generation | Function Calling |\n",
    "|-----------------------------------------------|---------------------|------------------|------------------|\n",
    "| doubao-1-5-pro-256k-250115                    |                     |                | ✅               |\n",
    "| doubao-1-5-thinking-pro-250415                |                     |                | ✅                 |\n",
    "| doubao-1-5-thinking-pro-m-250415              | ✅                  |                | ✅               |\n",
    "| doubao-1.5-vision-pro-250328                  | ✅                  |                  |                  |\n",
    "| doubao-seedance-1-0-lite-i2v-250428           |                     | ✅               |                  |\n",
    "| deepseek-r1-250120                            |                   |                  |✅                  |\n",
    "| deepseek-v3-250324                            |                   |                  |✅                  |\n",
    "| doubao-1-5-pro-32k-250115                     |                   |                  |✅                  |\n",
    "| doubao-1-5-lite-32k-250115                    |                   |                  |✅                  |\n",
    "\n",
    "The full API reference can be found [here](https://www.volcengine.com/docs/82379/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9d8bb9",
   "metadata": {},
   "source": [
    "## Text Capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fcf558",
   "metadata": {},
   "source": [
    "### Single-turn Chat\n",
    "\n",
    "Single-turn chat is the simplest form of interaction with a large language model. Single-turn chat generally without any context information. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ed9301",
   "metadata": {},
   "outputs": [],
   "source": [
    "from volcenginesdkarkruntime import Ark\n",
    "\n",
    "#\n",
    "# Create an Ark API client instance\n",
    "#\n",
    "client = Ark(api_key=ARK_API_KEY)\n",
    "\n",
    "#\n",
    "# Build messages\n",
    "#\n",
    "user_prompt = \"Who are you?\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": user_prompt},\n",
    "]\n",
    "\n",
    "#\n",
    "# Wait for LLM response\n",
    "#\n",
    "response = client.chat.completions.create(model=DEFAULT_LLM, messages=messages)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cca05f4",
   "metadata": {},
   "source": [
    "### Multi-turn Chat\n",
    "\n",
    "Generally, you can carry history information in multi-turn chat. History information includes user's historical messages and model's response. For example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11c45ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from volcenginesdkarkruntime import Ark\n",
    "\n",
    "#\n",
    "# Create an Ark API client instance\n",
    "#\n",
    "client = Ark(api_key=ARK_API_KEY)\n",
    "\n",
    "#\n",
    "# Build messages\n",
    "#\n",
    "user_prompt_1 = \"Your name is Bytedancer.\"  # for 1st round chat\n",
    "user_prompt_2 = \"Do you remember your name?\"  # for 2nd round chat\n",
    "\n",
    "#\n",
    "# The first turn chat\n",
    "#\n",
    "response = client.chat.completions.create(\n",
    "    model=DEFAULT_LLM,\n",
    "    messages=[{\"role\": \"user\", \"content\": user_prompt_1}],\n",
    ")\n",
    "content = response.choices[0].message.content\n",
    "\n",
    "#\n",
    "# The second turn chat\n",
    "#\n",
    "response = client.chat.completions.create(\n",
    "    model=DEFAULT_LLM,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": user_prompt_1},\n",
    "        {\"role\": \"assistant\", \"content\": content},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_2},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Q1: \" + user_prompt_1)\n",
    "print(\"A1: \" + content)\n",
    "print(\"Q2: \" + user_prompt_2)\n",
    "print(\"A2: \" + response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db40b92",
   "metadata": {},
   "source": [
    "### Stream Chat\n",
    "\n",
    "Stream chat (i.e., make model response to be streaming) can reduce the user's waiting time when the model's output is too long. You can enable stream chat by setting the `stream` as `True`, then the output will be printed gradually:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e817e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from volcenginesdkarkruntime import Ark\n",
    "\n",
    "#\n",
    "# Create an Ark API client instance\n",
    "#\n",
    "client = Ark(api_key=ARK_API_KEY)\n",
    "\n",
    "#\n",
    "# Build messages\n",
    "#\n",
    "user_prompt = (\n",
    "    \"Please help me to write an introduction of Bytedance with nearly 300 words.\"\n",
    ")\n",
    "messages = [{\"role\": \"user\", \"content\": user_prompt}]\n",
    "\n",
    "#\n",
    "# Wait for LLM response\n",
    "#\n",
    "stream = client.chat.completions.create(\n",
    "    model=DEFAULT_LLM,\n",
    "    messages=messages,\n",
    "    stream=True,  # streaming output\n",
    ")\n",
    "\n",
    "#\n",
    "# Print streaming output\n",
    "#\n",
    "for chunk in stream:\n",
    "    if not chunk.choices:\n",
    "        continue\n",
    "    print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e5362c",
   "metadata": {},
   "source": [
    "## Vision Capabilities\n",
    "\n",
    "ARK provides capabilities about multi-media, such as vision and sounds. Here we introduce the vision-related demos. The vision-related task is divided into image understanding and video generation:\n",
    "- Image understanding: this task can read information from one or several images and return the content to the user\n",
    "- Video generation: this task can generate video from text and images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf21d433",
   "metadata": {},
   "source": [
    "### Image understanding\n",
    "\n",
    "We use the default vision model to understand the following image:\n",
    "\n",
    "![demo_image](https://ark-tutorial.tos-cn-beijing.volces.com/assets/images/cat.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff58d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from volcenginesdkarkruntime import Ark\n",
    "\n",
    "#\n",
    "# Create an Ark API client instance\n",
    "#\n",
    "client = Ark(api_key=ARK_API_KEY)\n",
    "\n",
    "#\n",
    "# The image can be defined by a URL or a base64 string. For this demo, we use a Volcengine TOS url.\n",
    "#\n",
    "IMAGE_PATH = \"https://ark-tutorial.tos-cn-beijing.volces.com/assets/images/cat.png\"\n",
    "\n",
    "#\n",
    "# Build messages\n",
    "#\n",
    "user_prompt = \"Please describe this image with details.\"\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"text\": \"Please describe this image with details.\", \"type\": \"text\"},\n",
    "            {\"image_url\": {\"url\": IMAGE_PATH}, \"type\": \"image_url\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "#\n",
    "# Wait for VLM response\n",
    "#\n",
    "response = client.chat.completions.create(model=DEFAULT_VLM, messages=messages)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f184b5f",
   "metadata": {},
   "source": [
    "### Video generation\n",
    "\n",
    "The following demo shows generating a video according to a static image and prompt.\n",
    "\n",
    "The video generation is asynchronous, hence the generation goes through two stages:\n",
    "1. Send generation request\n",
    "   - Input: prompt, image (optional), and other parameters\n",
    "   - Output: generation task ID\n",
    "2. Check the status of the generation\n",
    "\n",
    "The entire process is shown in the following code snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48604e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from volcenginesdkarkruntime import Ark\n",
    "\n",
    "#\n",
    "# Create an Ark API client instance\n",
    "#\n",
    "client = Ark(api_key=ARK_API_KEY)\n",
    "\n",
    "#\n",
    "# Build messages\n",
    "#\n",
    "user_prompt = \"Please generate a video with a cat running. --ratio 16:9\"\n",
    "messages = [{\"type\": \"text\", \"text\": user_prompt}]\n",
    "\n",
    "#\n",
    "# Send a video generation request\n",
    "#\n",
    "print(\"1. Send generation request\")\n",
    "response = client.content_generation.tasks.create(\n",
    "    model=VIDEO_GENERATION_LM, content=messages\n",
    ")\n",
    "tid = response.id\n",
    "print(f\"Video generation task {tid} submitted.\")\n",
    "\n",
    "#\n",
    "# Check the status of the submitted task\n",
    "#\n",
    "print(\"2. Check the status of the generation\")\n",
    "MAX_RETRIES = 100\n",
    "for _ in range(MAX_RETRIES):\n",
    "    response = client.content_generation.tasks.get(task_id=tid)\n",
    "    status = response.status\n",
    "\n",
    "    if status == \"succeeded\":\n",
    "        print(\n",
    "            f\"Successfully! Your video can be download from {response.content.video_url}\"\n",
    "        )\n",
    "        break\n",
    "    else:\n",
    "        print(f\"Current status: {status}\")\n",
    "\n",
    "    time.sleep(10)  # check every 10 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ff7215",
   "metadata": {},
   "source": [
    " For more models that support video generation, you can visit [here](https://www.volcengine.com/docs/82379/1366799#%E6%94%AF%E6%8C%81%E6%A8%A1%E5%9E%8B).\n",
    " \n",
    " If you want to make the video more vivid, maybe you need [prompt refine](https://www.promptrefine.com/prompt/new)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a675bd4",
   "metadata": {},
   "source": [
    "# Agent\n",
    "\n",
    "Here we introduce the architecture and key concepts of Arkitect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc3d9fc",
   "metadata": {},
   "source": [
    "## Minimal Agent\n",
    "\n",
    "You can build a minimal agent through the following code. In Arkitect, the `context` represents an agentic instance, which can sent the user's prompt to LLM and response the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a3b796",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arkitect.core.component.context.context import Context\n",
    "\n",
    "#\n",
    "# Initialize context\n",
    "#\n",
    "ctx = Context(model=DEFAULT_LLM)\n",
    "await ctx.init()\n",
    "\n",
    "#\n",
    "# Build messages\n",
    "#\n",
    "system_prompt = \"You are a Customer service, you can answer the user's questions as a customer service agent.\"\n",
    "user_prompt = \"I bought a product, but it's not working. Can you help me?\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt},\n",
    "]\n",
    "\n",
    "#\n",
    "# Wait for agent responses\n",
    "#\n",
    "response = await ctx.completions.create(messages=messages, stream=False)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42fbce6",
   "metadata": {},
   "source": [
    "## Tool\n",
    "\n",
    "Agent uses a tool by *function calling* to finish a task. Function calling is a way to call a function in the agent's mind. The agent can call a function with the name and arguments. The agent can also call multiple functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17358fd1",
   "metadata": {},
   "source": [
    "### Function Tool\n",
    "\n",
    "We use python to build a flight searching tool. Note that the docstring is necessary for the tool to work, as the LLM will learn the function from the docstring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff473050",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_flight(source: str, destination: str) -> str:\n",
    "    \"\"\"Search flight from source to destination.\n",
    "    Args:\n",
    "        source (str): The source city.\n",
    "        destination (str): The destination city.\n",
    "    Returns:\n",
    "        str: The flight information.\n",
    "    \"\"\"\n",
    "    print(\"------ Tool: search_flight ------\")\n",
    "\n",
    "    mock_flight = {\n",
    "        \"source\": source,\n",
    "        \"destination\": destination,\n",
    "        \"price\": 1000,\n",
    "        \"departure_time\": \"2025-07-13 04:21\",\n",
    "        \"arrival_time\": \"2025-07-13 20:34\",\n",
    "        \"flight_number\": \"BYTEDANCE 888\",\n",
    "        \"airline\": \"Bytedance Airlines\",\n",
    "        \"aircraft\": \"Boeing 737\",\n",
    "    }\n",
    "    return str(mock_flight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8463d50a",
   "metadata": {},
   "source": [
    "Then we can equip an agent with the above tool by passing the `search_flight` to `tools` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87329e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arkitect.core.component.context.context import Context\n",
    "\n",
    "#\n",
    "# Initialize context with `search_flight` tool\n",
    "#\n",
    "ctx = Context(model=DEFAULT_LLM, tools=[search_flight])\n",
    "await ctx.init()\n",
    "\n",
    "#\n",
    "# Build messages\n",
    "#\n",
    "user_prompt = \"Help me to check the flight from Beijing to Singapore.\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": user_prompt},\n",
    "]\n",
    "\n",
    "#\n",
    "# Wait for agent responses\n",
    "#\n",
    "response = await ctx.completions.create(messages=messages, stream=False)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58553c22",
   "metadata": {},
   "source": [
    "### Build-in Tool\n",
    "\n",
    "The ARK will provide some built-in tools to finish common tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a7ec0e",
   "metadata": {},
   "source": [
    "#### Link Reader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3991c9e6",
   "metadata": {},
   "source": [
    "The code is used for check web information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05083b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arkitect.core.component.context.context import Context\n",
    "from arkitect.core.component.tool.builtin_tools import link_reader\n",
    "\n",
    "#\n",
    "# Initialize context with `link_reader` tool, this tool is implemented by Arkitect, so you don't need to implement it by yourself.\n",
    "#\n",
    "ctx = Context(model=DEFAULT_LLM, tools=[link_reader])\n",
    "await ctx.init()\n",
    "\n",
    "#\n",
    "# Build messages\n",
    "#\n",
    "user_prompt = (\n",
    "    \"Tell me the top 3 news of bytedance from https://www.bytedance.com/zh/news\"\n",
    ")\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": user_prompt},\n",
    "]\n",
    "\n",
    "#\n",
    "# Wait for agent responses\n",
    "#\n",
    "response = await ctx.completions.create(messages=messages, stream=False)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baec13ea",
   "metadata": {},
   "source": [
    "#### Calculator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18147132",
   "metadata": {},
   "source": [
    "The calculator can realize simple mathematical operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08358ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arkitect.core.component.context.context import Context\n",
    "from arkitect.core.component.tool.builtin_tools import calculator\n",
    "\n",
    "#\n",
    "# initialize context with `calculator` tool, this tool is implemented by Arkitect, so you don't need to implement it by yourself.\n",
    "#\n",
    "ctx = Context(model=DEFAULT_LLM, tools=[calculator])\n",
    "await ctx.init()\n",
    "\n",
    "#\n",
    "# Build messages\n",
    "#\n",
    "user_prompt = \"What is the result of (12345*67890+999)/999\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": user_prompt},\n",
    "]\n",
    "\n",
    "#\n",
    "# Wait for agent responses\n",
    "#\n",
    "response = await ctx.completions.create(messages=messages, stream=False)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef562b8",
   "metadata": {},
   "source": [
    "### MCP\n",
    "\n",
    "We can connect a MCP server to use its tool. Existing mcp-server generally has two modes:\n",
    "\n",
    "- `stdio`: No service needed. Once a tool is called, a new process is started to run the tool.\n",
    "- `sse`: Needs a service with a host and a port to provide the sse interface. Once a tool is called, a new connection is established & a request is sent to the service.\n",
    "\n",
    "Before we perform the above two modes to show MCP tool calling, we should prepare some basic configs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96cd178",
   "metadata": {},
   "outputs": [],
   "source": [
    "MCP_HOST = \"127.0.0.1\"\n",
    "MCP_SSE_PORT = 8081\n",
    "MCP_STREAMABLE_HTTP_PORT = 8082"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b11f5e",
   "metadata": {},
   "source": [
    "#### Stdio mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8261f482",
   "metadata": {},
   "source": [
    "**MCP Server**\n",
    "\n",
    "The server is not activated until it is called by a MCP client. Ensure you have installed the `mcp_server_time` package:\n",
    "\n",
    "```bash\n",
    "pip install mcp_server_time\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075faba4",
   "metadata": {},
   "source": [
    "**MCP Client**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b0af3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arkitect.core.component.context.context import Context\n",
    "from arkitect.core.component.tool.mcp_client import MCPClient\n",
    "\n",
    "#\n",
    "# Initialize a mcp client, the server is activated by executing the `command` and `arguments` in a new process\n",
    "#\n",
    "mcp_client = MCPClient(\n",
    "    name=\"TimeTools\",\n",
    "    command=\"python\",\n",
    "    arguments=[\"-m\", \"mcp_server_time\", \"--local-timezone\", \"Asia/Shanghai\"],\n",
    "    errlog=None,  # This item needs to be set in Colab, but it is not required for local running.\n",
    ")\n",
    "\n",
    "#\n",
    "# Initialize context with the mcp client\n",
    "#\n",
    "ctx = Context(model=DEFAULT_LLM, tools=[mcp_client])\n",
    "await ctx.init()\n",
    "\n",
    "#\n",
    "# Build messages\n",
    "#\n",
    "user_prompt = \"What time is it in Beijing time now?\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": user_prompt},\n",
    "]\n",
    "\n",
    "#\n",
    "# Wait for agent responses\n",
    "#\n",
    "response = await ctx.completions.create(messages=messages, stream=False)\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "#\n",
    "# Clean up async resources (best practice)\n",
    "#\n",
    "await mcp_client.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d0726a",
   "metadata": {},
   "source": [
    "#### SSE mode\n",
    "\n",
    "Needs a backend service with a specific host and port."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94a9705",
   "metadata": {},
   "source": [
    "**MCP Server**\n",
    "\n",
    "We use `FastMCP` to start a mcp server with `sse` transport method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a4110e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process\n",
    "from fastmcp import FastMCP\n",
    "import random\n",
    "\n",
    "#\n",
    "# Initialize a mcp server instance\n",
    "#\n",
    "mcp = FastMCP(\"WeatherService\", port=MCP_SSE_PORT)\n",
    "\n",
    "\n",
    "#\n",
    "# Add `get_weather` tool to the mcp server\n",
    "#\n",
    "@mcp.tool()\n",
    "def get_weather(city: str) -> dict:\n",
    "    \"\"\"Get the current weather of the specified city\n",
    "\n",
    "    Args:\n",
    "        city (str): the city name\n",
    "\n",
    "    Returns:\n",
    "        dict: the weather information of the city\n",
    "    \"\"\"\n",
    "    cities = [\"Beijing\", \"Shanghai\", \"Guangzhou\", \"Shenzhen\", \"Hong Kong\"]\n",
    "    if city not in cities:\n",
    "        return {\n",
    "            \"message\": \"Unable to find weather information for the city\",\n",
    "            \"city\": city,\n",
    "        }\n",
    "    return {\n",
    "        \"message\": \"success\",\n",
    "        \"city\": city,\n",
    "        \"temperature\": random.randint(10, 30),\n",
    "        \"condition\": [\"sunny\", \"cloudy\", \"rainy\"][random.randint(0, 2)],\n",
    "        \"unit\": \"celsius\",\n",
    "    }\n",
    "\n",
    "\n",
    "#\n",
    "# Initialize and start a mcp server thread as a daemon process\n",
    "#\n",
    "mcp_server = Process(target=lambda: mcp.run(transport=\"sse\"))\n",
    "mcp_server.daemon = True\n",
    "mcp_server.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60103708",
   "metadata": {},
   "source": [
    "**MCP Client**\n",
    "\n",
    "Set the `server_url` in mcp client initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953c07b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arkitect.core.component.context.context import Context\n",
    "from arkitect.core.component.tool.mcp_client import MCPClient\n",
    "\n",
    "#\n",
    "# Initialize a mcp server instance\n",
    "#\n",
    "mcp_client = MCPClient(\n",
    "    name=\"weather_mcp_client\", server_url=f\"http://{MCP_HOST}:{MCP_SSE_PORT}/sse\"\n",
    ")\n",
    "\n",
    "#\n",
    "# Initialize context with the mcp client\n",
    "#\n",
    "ctx = Context(model=DEFAULT_LLM, tools=[mcp_client])\n",
    "await ctx.init()\n",
    "\n",
    "#\n",
    "# Build messages\n",
    "#\n",
    "user_prompt = \"What's the weather like in Beijing?\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": user_prompt},\n",
    "]\n",
    "\n",
    "#\n",
    "# Wait for agent responses\n",
    "#\n",
    "response = await ctx.completions.create(messages=messages, stream=False)\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "#\n",
    "# Clean up async resources (best practice)\n",
    "#\n",
    "await mcp_client.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0df4b4",
   "metadata": {},
   "source": [
    "#### Streamable-http mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91ea7ad",
   "metadata": {},
   "source": [
    "**MCP Server**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54e13a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Process\n",
    "from fastmcp import FastMCP\n",
    "import random\n",
    "\n",
    "#\n",
    "# Initialize a mcp server instance\n",
    "#\n",
    "mcp = FastMCP(\"Weather Service\", port=MCP_STREAMABLE_HTTP_PORT)\n",
    "\n",
    "\n",
    "#\n",
    "# Add `get_weather` tool to the mcp server\n",
    "#\n",
    "@mcp.tool()\n",
    "def get_weather(city: str) -> dict:\n",
    "    \"\"\"Get the current weather of the specified city\n",
    "\n",
    "    Args:\n",
    "        city (str): the city name\n",
    "\n",
    "    Returns:\n",
    "        dict: the weather information of the city\n",
    "    \"\"\"\n",
    "    cities = [\"Beijing\", \"Shanghai\", \"Guangzhou\", \"Shenzhen\", \"Hong Kong\"]\n",
    "    if city not in cities:\n",
    "        return {\n",
    "            \"message\": \"Unable to find weather information for the city\",\n",
    "            \"city\": city,\n",
    "        }\n",
    "    return {\n",
    "        \"message\": \"success\",\n",
    "        \"city\": city,\n",
    "        \"temperature\": random.randint(10, 30),\n",
    "        \"condition\": [\"sunny\", \"cloudy\", \"rainy\"][random.randint(0, 2)],\n",
    "        \"unit\": \"celsius\",\n",
    "    }\n",
    "\n",
    "\n",
    "#\n",
    "# Initialize and start a mcp server thread as a daemon process\n",
    "#\n",
    "mcp_server = Process(target=lambda: mcp.run(transport=\"streamable-http\"))\n",
    "mcp_server.daemon = True\n",
    "mcp_server.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd46b48",
   "metadata": {},
   "source": [
    "**MCP Client**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fd5e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arkitect.core.component.context.context import Context\n",
    "from arkitect.core.component.tool.mcp_client import MCPClient\n",
    "\n",
    "#\n",
    "# Initialize a mcp server instance\n",
    "#\n",
    "mcp_client = MCPClient(\n",
    "    name=\"weather_mcp_client\",\n",
    "    server_url=f\"http://{MCP_HOST}:{MCP_STREAMABLE_HTTP_PORT}/mcp\",\n",
    "    transport=\"streamable-http\",\n",
    ")\n",
    "\n",
    "#\n",
    "# Initialize context with the mcp client\n",
    "#\n",
    "ctx = Context(model=DEFAULT_LLM, tools=[mcp_client])\n",
    "await ctx.init()\n",
    "\n",
    "#\n",
    "# Build messages\n",
    "#\n",
    "user_prompt = \"What's the weather like in Beijing?\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": user_prompt},\n",
    "]\n",
    "\n",
    "#\n",
    "# Wait for agent responses\n",
    "#\n",
    "response = await ctx.completions.create(messages=messages, stream=False)\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "#\n",
    "# Clean up async resources (best practice)\n",
    "#\n",
    "await mcp_client.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79436f2a",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a1635e4",
   "metadata": {},
   "source": [
    "### Knowledge Base\n",
    "\n",
    "We provide two methods to implement vector database:\n",
    "\n",
    "- `chromadb` (a typical memory vector database) deployed locally\n",
    "- `OpenSearch` deployed on Volcengine ECS\n",
    "\n",
    "For our test data, we prepare a list of event happened in some years:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b028561",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"In 1936, Alan Turing proposed the Turing machine model, laying the theoretical foundation for modern computers;\",\n",
    "    \"In 1949, Maurice Wilkes completed EDSAC, the first electronic computer to implement the stored-program concept.\",\n",
    "    \"In 1957, John Backus and his team developed FORTRAN, the first widely used high-level programming language.\",\n",
    "    \"In 1965, Gordon Moore proposed Moore's Law, predicting that the number of transistors in integrated circuits would double approximately every two years.\",\n",
    "    \"In 1969, Ken Thompson and Dennis Ritchie developed the Unix operating system at Bell Labs, which was written in the C programming language.\",\n",
    "    \"In 1984, Richard Stallman released the GNU General Public License (GPL), driving the free software movement.\",\n",
    "    \"In 1991, Linus Torvalds created the Linux kernel, which was released under the GPL license.\",\n",
    "    \"In 2000, Fabrice Bellard developed FFmpeg, an open-source multimedia framework supporting audio/video codecs and streaming processing.\",\n",
    "    \"In 2012, Geoffrey Hinton's team used the deep convolutional network AlexNet in the ImageNet competition, sparking the resurgence of deep learning.\",\n",
    "    \"In 2017, Ashish Vaswani and colleagues published the paper *Attention Is All You Need*, introducing the Transformer architecture that revolutionized natural language processing.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd938929",
   "metadata": {},
   "source": [
    "#### With ChromaDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6863c678",
   "metadata": {},
   "source": [
    "**Initialization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f70269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "COLLECTION_NAME = \"test_chromadb_vectordb\"\n",
    "\n",
    "#\n",
    "# Initialize `chroma`` client and create a collection (table in typical database)\n",
    "#\n",
    "chroma_client = chromadb.Client()\n",
    "collection = chroma_client.create_collection(COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444deb5a",
   "metadata": {},
   "source": [
    "**Embedding**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a67a601",
   "metadata": {},
   "source": [
    "Then we embed the text to vertors using the *embedding model*. The introduction and preliminaries can be found [here](https://www.volcengine.com/docs/82379/1329508)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c66cd9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from volcenginesdkarkruntime import Ark\n",
    "\n",
    "#\n",
    "# Create a ARK API client\n",
    "#\n",
    "client = Ark(api_key=ARK_API_KEY)\n",
    "\n",
    "#\n",
    "# Request for embedding generation\n",
    "#\n",
    "response = client.embeddings.create(model=EMBEDDING_MODEL, input=documents)\n",
    "embeddings = [response.data[i].embedding for i in range(len(response.data))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c23b16",
   "metadata": {},
   "source": [
    "**Indexing**\n",
    "\n",
    "The embedding text should be added into the collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487598e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Add the embeded results to collection, the collection will build index automatically\n",
    "#\n",
    "collection.add(\n",
    "    ids=[str(i) for i in range(len(documents))],\n",
    "    documents=documents,\n",
    "    embeddings=embeddings,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba47730",
   "metadata": {},
   "source": [
    "**Search function**\n",
    "\n",
    "Build a search interface to search for a specific string in a text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d7cf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Retrieve the top N most similar documents from vector database\n",
    "#\n",
    "TOP_N = 3\n",
    "\n",
    "\n",
    "#\n",
    "# Definition of processing query\n",
    "#\n",
    "def search(query: str) -> list[str]:\n",
    "    \"\"\"Retrieve documents similar to the query text in the vector database.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query text to be retrieved (e.g., \"Who proposed the Turing machine model?\")\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of the top most similar document contents retrieved (sorted by vector similarity)\n",
    "    \"\"\"\n",
    "    # Request for embedding the input string to vector\n",
    "    query_vector = client.embeddings.create(model=EMBEDDING_MODEL, input=[query])\n",
    "\n",
    "    # Query the vector database with the embedding vector\n",
    "    results = collection.query(\n",
    "        query_embeddings=query_vector.data[0].embedding, n_results=TOP_N\n",
    "    )\n",
    "    return results[\"documents\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a340394",
   "metadata": {},
   "source": [
    "#### With OpenSearch\n",
    "\n",
    "Before using OpenSearch on Volcengine, you need to create an ECS instance with OpenSearch framework. You can follow [here](https://www.volcengine.com/docs/6465/1117829) to finish prepartions.\n",
    "\n",
    "After that, set the following environment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f961d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# OpenSearch ECS related configurations\n",
    "#\n",
    "OPENSEARCH_HOST = \"YOUR_OPENSEARCH_HOST\"  # e.g., xxxx.escloud.volces.com\n",
    "OPENSEARCH_PORT = \"YOUR_OPENSEARCH_PORT\"\n",
    "\n",
    "OPENSEARCH_USER = \"YOUR_OPENSEARCH_USER\"\n",
    "OPENSEARCH_PASSWORD = \"YOUR_OPENSEARCH_PASSWORD\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e15d42d",
   "metadata": {},
   "source": [
    "**Initialization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0d6b0f",
   "metadata": {},
   "source": [
    "First we prepare some functions to operate the OpenSearch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eada1b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from volcenginesdkarkruntime import Ark\n",
    "\n",
    "\n",
    "#\n",
    "# A utility function for embedding text\n",
    "#\n",
    "def embed_text(document: str):\n",
    "    embedding_client = Ark(api_key=ARK_API_KEY)\n",
    "\n",
    "    response = embedding_client.embeddings.create(\n",
    "        model=EMBEDDING_MODEL, input=[document]\n",
    "    )\n",
    "    embeddings = response.data[0].embedding\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "#\n",
    "# Create a collection in opensearch\n",
    "#\n",
    "def opensearch_create_collection(collection_name: str, embedding_dim: int):\n",
    "    # Collection field schema\n",
    "    # This demo collection has two fields: `text` for storing the original text, and `embedding` for storing the embeded vector.\n",
    "    field_shcema = {\n",
    "        \"text\": {\n",
    "            \"type\": \"text\",\n",
    "        },\n",
    "        \"embedding\": {\n",
    "            \"type\": \"knn_vector\",\n",
    "            \"dimension\": embedding_dim,\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Create a index with fields\n",
    "    request_body = {\n",
    "        \"mappings\": {\"properties\": field_shcema},\n",
    "        \"settings\": {\n",
    "            \"index\": {\"knn\": True}\n",
    "        },  # Note: This config is required to enable vector (knn) search\n",
    "    }\n",
    "\n",
    "    # Send request to opensearch\n",
    "    response = opensearch_client.indices.create(\n",
    "        index=collection_name, body=request_body\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "#\n",
    "# Add text to opensearch collection\n",
    "#\n",
    "def opensearch_add_documents(collection_name: str, documents: list[str]):\n",
    "    documents_length = len(documents)\n",
    "    for i, document in enumerate(documents):\n",
    "        # Generate embedding for document\n",
    "        document_embedding = embed_text(document)\n",
    "\n",
    "        # Build request body\n",
    "        request_body = {\n",
    "            \"text\": document,\n",
    "            \"embedding\": document_embedding,\n",
    "        }\n",
    "        opensearch_client.index(index=collection_name, body=request_body, id=i)\n",
    "        print(f\"Add {i}/{documents_length} document to collection {collection_name}\")\n",
    "\n",
    "    # Note: Must refresh the index to make the newly added documents searchable.\n",
    "    opensearch_client.indices.refresh(index=collection_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda62c5f",
   "metadata": {},
   "source": [
    "Then we initialize a OpenSearch client with a collection, and upload the documents to the collection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf700c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from opensearchpy import OpenSearch\n",
    "\n",
    "COLLECTION_NAME = \"ark-sample-opensearch-collection\"  # aka. index name\n",
    "\n",
    "opensearch_client = OpenSearch(\n",
    "    hosts=[f\"{OPENSEARCH_HOST}:{OPENSEARCH_PORT}\"],\n",
    "    http_auth=(OPENSEARCH_USER, OPENSEARCH_PASSWORD),\n",
    "    use_ssl=True,\n",
    "    verify_certs=False,\n",
    "    ssl_show_warn=False,\n",
    ")\n",
    "\n",
    "#\n",
    "# Create a collection, the `embedding_dim` should be the same as the embedding model\n",
    "# here, we use `doubao-embedding-text-240715` which is 2560-dimension.\n",
    "#\n",
    "response = opensearch_create_collection(\n",
    "    collection_name=COLLECTION_NAME, embedding_dim=2560\n",
    ")\n",
    "print(\"Create collection done.\")\n",
    "\n",
    "#\n",
    "# Add documents to the collection\n",
    "#\n",
    "response = opensearch_add_documents(\n",
    "    collection_name=COLLECTION_NAME, documents=documents\n",
    ")\n",
    "print(\"Add all documents done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ced317",
   "metadata": {},
   "source": [
    "**Search function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2548cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Retrieve the top N most similar documents from vector database\n",
    "#\n",
    "TOP_N = 3\n",
    "\n",
    "\n",
    "#\n",
    "# Search function for OpenSearch\n",
    "#\n",
    "def opensearch_search(collection_name: str, query: str, top_k: int):\n",
    "    query_embedding = embed_text(query)\n",
    "\n",
    "    query_body = {\n",
    "        \"size\": top_k,\n",
    "        \"query\": {\"knn\": {\"embedding\": {\"vector\": query_embedding, \"k\": top_k}}},\n",
    "    }\n",
    "    response = opensearch_client.search(index=collection_name, body=query_body)\n",
    "\n",
    "    # Format search result\n",
    "    # The original format of search result is as follows:\n",
    "    # {\n",
    "    #     ...\n",
    "    #     \"hits\": {\n",
    "    #         \"hits\": [\n",
    "    #             {\n",
    "    #                 ...\n",
    "    #                 \"_source\": {\n",
    "    #                     'text': ...\n",
    "    #                     'embedding': ...\n",
    "    #                 }\n",
    "    #             }\n",
    "    #         ]\n",
    "    # }\n",
    "    documents = []\n",
    "    for hit in response[\"hits\"][\"hits\"]:\n",
    "        document = hit[\"_source\"][\"text\"]\n",
    "        documents.append(document)\n",
    "    return documents\n",
    "\n",
    "\n",
    "#\n",
    "# Search function for agent\n",
    "#\n",
    "def search(query: str) -> list[str]:\n",
    "    \"\"\"Retrieve documents similar to the query text in the vector database.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query text to be retrieved (e.g., \"Who proposed the Turing machine model?\")\n",
    "\n",
    "    Returns:\n",
    "        list[str]: A list of the top most similar document contents retrieved (sorted by vector similarity)\n",
    "    \"\"\"\n",
    "    results = opensearch_search(\n",
    "        collection_name=COLLECTION_NAME, query=query, top_k=TOP_N\n",
    "    )\n",
    "    return str(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7136c366",
   "metadata": {},
   "source": [
    "### Equip to Agent\n",
    "\n",
    "The knowledge base should be equipped to enable RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5dc666",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arkitect.core.component.context.context import Context\n",
    "\n",
    "#\n",
    "# Initialize context and equip it with `search` function\n",
    "#\n",
    "ctx = Context(model=DEFAULT_LLM, tools=[search])\n",
    "await ctx.init()\n",
    "\n",
    "#\n",
    "# Build messages\n",
    "#\n",
    "user_prompt = \"What did Hinton and his team do in 2012?\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": user_prompt},\n",
    "]\n",
    "\n",
    "#\n",
    "# Wait for agent responses\n",
    "#\n",
    "response = await ctx.completions.create(messages=messages, stream=False)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576fb8b7",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbee995b",
   "metadata": {},
   "source": [
    "### Sequential\n",
    "\n",
    "You can use a simple `for` loop to realize sequential operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacbb05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arkitect.core.component.context.context import Context\n",
    "\n",
    "#\n",
    "# Define three steps to finish a task\n",
    "#\n",
    "steps = [\n",
    "    {\n",
    "        \"name\": \"Poet\",\n",
    "        \"description\": \"Write a Chinese poem (two sentence, each sentence has seven words) to describe spring. You should return the poem in Chinese without any other outputs.\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Poem Evaluator\",\n",
    "        \"description\": \"Evaluate a poem generated by LLM. You should return the original poem and its optimization suggestions in Chinese without any other outputs.\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Poem Optimizer\",\n",
    "        \"description\": \"Optimize the Chinese poem according to original version and its optimization suggestions. You should return the optimized poem in Chinese without any other outputs.\",\n",
    "    },\n",
    "]\n",
    "\n",
    "#\n",
    "# Initialize context\n",
    "#\n",
    "ctx = Context(model=DEFAULT_LLM)\n",
    "await ctx.init()\n",
    "\n",
    "#\n",
    "# Use `for` to iterate over steps\n",
    "#\n",
    "input = \"\"\n",
    "for step in steps:\n",
    "    # Build messages\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": f\"You are a intelligent {step['name']}.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Please complete the task: {step['description']}. {input}\",\n",
    "        },\n",
    "    ]\n",
    "    # Wait for agent response\n",
    "    response = await ctx.completions.create(messages=messages, stream=False)\n",
    "    input = response.choices[0].message.content\n",
    "    print(f\"Output from {step['name']}:\\n{input}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7fa0ff5",
   "metadata": {},
   "source": [
    "### Parallel\n",
    "\n",
    "You can use `async.gather` to run multiple tasks in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070d21f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from arkitect.core.component.context.context import Context\n",
    "\n",
    "#\n",
    "# Define three tasks\n",
    "#\n",
    "tasks = [\n",
    "    {\n",
    "        \"name\": \"Flight Planner\",\n",
    "        \"description\": \"Mock just one flight information four factors: flight number, time, location, and price\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Accommodation Planner\",\n",
    "        \"description\": \"Mock just one accommodation information four factors: accommodation name, location, price, and rating\",\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Activity Planner\",\n",
    "        \"description\": \"Mock just one activity information four factors: activity name, simple introduction, and the reason why you recommend it\",\n",
    "    },\n",
    "]\n",
    "\n",
    "#\n",
    "# Initialize context\n",
    "#\n",
    "ctx = Context(model=DEFAULT_LLM)\n",
    "await ctx.init()\n",
    "\n",
    "#\n",
    "# Use `for` to iterate over steps\n",
    "#\n",
    "input = \"Beijing to Singapore, from 2025/03/01 to 2025/03/10\"\n",
    "message_group = [\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"You are a {task['name']}, your task is {task['description']}. You just need to output pure text without Markdown formats.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"My trip: {input}\",\n",
    "        },\n",
    "    ]\n",
    "    for task in tasks\n",
    "]\n",
    "\n",
    "#\n",
    "# Use `asyncio.gather` to run tasks concurrently\n",
    "#\n",
    "res = await asyncio.gather(\n",
    "    ctx.completions.create(messages=message_group[0], stream=False),\n",
    "    ctx.completions.create(messages=message_group[1], stream=False),\n",
    "    ctx.completions.create(messages=message_group[2], stream=False),\n",
    ")\n",
    "\n",
    "#\n",
    "# Check outputs\n",
    "#\n",
    "for task in tasks:\n",
    "    print(\n",
    "        f\"Output from {task['name']}:\\n{res[tasks.index(task)].choices[0].message.content}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6704fd8c",
   "metadata": {},
   "source": [
    "## Human-in-the-loop\n",
    "\n",
    "The human-in-the-loop interrupt is implemented as a `ApprovalHook`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c400a5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arkitect.core.component.context.context import Context\n",
    "from arkitect.core.component.context.hooks import ApprovalHook\n",
    "\n",
    "#\n",
    "# Initialize context with `ApprovalHook`\n",
    "#\n",
    "ctx = Context(model=DEFAULT_LLM, tools=[search_flight])\n",
    "ctx.set_pre_tool_call_hook(ApprovalHook())  # add the approval hook in pre-tool-call\n",
    "await ctx.init()\n",
    "\n",
    "#\n",
    "# Build messages\n",
    "#\n",
    "user_prompt = \"Help me to check the flight from Beijing to Singapore.\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": user_prompt},\n",
    "]\n",
    "\n",
    "#\n",
    "# Wait for the agent response\n",
    "#\n",
    "response = await ctx.completions.create(messages=messages, stream=False)\n",
    "\n",
    "#\n",
    "# Check outputs and handle reject case\n",
    "#\n",
    "if hasattr(response, \"choices\"):\n",
    "    print(response.choices[0].message.content)\n",
    "else:\n",
    "    print(\"User rejects tool call.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3e0186",
   "metadata": {},
   "source": [
    "# [WIP] Samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31352310",
   "metadata": {},
   "source": [
    "## Custom service\n",
    "\n",
    "This demo shows how to create a custom service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bacf53e",
   "metadata": {},
   "source": [
    "### Definition\n",
    "\n",
    "**Task**: Receive a message from a user and send a response according to preset question/answer pairs.\n",
    "\n",
    "**Input**: A message from a user.\n",
    "\n",
    "**Output**: A response to the user's message."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2c7a36",
   "metadata": {},
   "source": [
    "### Workflow\n",
    "\n",
    "1. Receive user's message\n",
    "2. Retrieve relevant documents from knowledge base (i.e.,vector database)\n",
    "3. Generate a response using Doubao LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60133c0",
   "metadata": {},
   "source": [
    "### Components\n",
    "\n",
    "**Knowledge base**: A collection of question/answer pairs.\n",
    "\n",
    "**Tools**: `xxx`, `xxx`, and `xxx` tools for xxx."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7eabb5",
   "metadata": {},
   "source": [
    "### Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3771da",
   "metadata": {},
   "source": [
    "**Build knowledge base**\n",
    "\n",
    "We build a knowledge base from the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108cfdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build something"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d6dcd1",
   "metadata": {},
   "source": [
    "## Information summarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64061cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b0374d",
   "metadata": {},
   "source": [
    "## Recommendation engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e977fa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f35800c",
   "metadata": {},
   "source": [
    "## Platform monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3468dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8d7a1b",
   "metadata": {},
   "source": [
    "# [WIP] Agent Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f024e266",
   "metadata": {},
   "source": [
    "## Callbacks\n",
    "\n",
    "Sometimes, we wanna trace the agent execution to examine the behavior. We can use the `callback` to do this. The `callback` is a function that will be called in a specific situation.\n",
    "\n",
    "There are 4 callbacks provided in ARK:\n",
    "\n",
    "- `PreLLMCallHook`: Called before making any LLM API call\n",
    "- `PostLLMCallHook`: Called after receiving response from LLM\n",
    "- `PreToolCallHook`: Called before executing any tool function\n",
    "- `PostToolCallHook`: Called after tool function execution\n",
    "\n",
    "Each callback is implemented as a *hook*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0f19d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from arkitect.core.component.context.context import Context\n",
    "from arkitect.core.component.context.hooks import (\n",
    "    PostLLMCallHook,\n",
    "    PostToolCallHook,\n",
    "    PreLLMCallHook,\n",
    "    PreToolCallHook,\n",
    ")\n",
    "\n",
    "\n",
    "#\n",
    "# Implement 4 hooks to realize callbacks\n",
    "#\n",
    "class SimplePreLLMCallHook(PreLLMCallHook):\n",
    "    async def pre_llm_call(self, state):\n",
    "        print(\"Pre LLM Call Hook: Executed before LLM call\")\n",
    "        return state\n",
    "\n",
    "\n",
    "class SimplePostLLMCallHook(PostLLMCallHook):\n",
    "    async def post_llm_call(self, state):\n",
    "        print(\"Post LLM Call Hook: Executed after LLM call\")\n",
    "        return state\n",
    "\n",
    "\n",
    "class SimplePreToolCallHook(PreToolCallHook):\n",
    "    async def pre_tool_call(self, name: str, arguments: str, state):\n",
    "        print(f\"Pre Tool Call Hook: Executed before tool {name} call\")\n",
    "        return state\n",
    "\n",
    "\n",
    "class SimplePostToolCallHook(PostToolCallHook):\n",
    "    async def post_tool_call(\n",
    "        self,\n",
    "        name: str,\n",
    "        arguments: str,\n",
    "        response,\n",
    "        exception,\n",
    "        state,\n",
    "    ):\n",
    "        print(f\"Post Tool Call Hook: Executed after tool {name} call\")\n",
    "        return state\n",
    "\n",
    "\n",
    "#\n",
    "# Initialize context with the 4 hooks\n",
    "#\n",
    "context = Context(\n",
    "    model=DEFAULT_LLM,\n",
    ")\n",
    "context.set_pre_llm_call_hook(SimplePreLLMCallHook())\n",
    "context.set_post_llm_call_hook(SimplePostLLMCallHook())\n",
    "context.set_pre_tool_call_hook(SimplePreToolCallHook())\n",
    "context.set_post_tool_call_hook(SimplePostToolCallHook())\n",
    "await context.init()\n",
    "\n",
    "#\n",
    "# Build messages\n",
    "#\n",
    "user_prompt = \"Ping!\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": user_prompt},\n",
    "]\n",
    "\n",
    "#\n",
    "# Wait for agent responses\n",
    "#\n",
    "response = await context.completions.create(messages=messages, stream=False)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ec7cf5",
   "metadata": {},
   "source": [
    "## Tracing\n",
    "\n",
    "You can trace your agent actions by tracing module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef5409b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af54f873",
   "metadata": {},
   "source": [
    "## Performance\n",
    "\n",
    "Evaluate the execution latency, throughput, and resource utilization of the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32267002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb7de02",
   "metadata": {},
   "source": [
    "# Compatibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f37abe",
   "metadata": {},
   "source": [
    "## OpenAI API\n",
    "\n",
    "Reference [here](https://www.volcengine.com/docs/82379/1330626)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arkintelligence",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
